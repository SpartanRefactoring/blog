\subsection{Data Corpus}
We used the ‟Gil-Lalouche”~\cite{Gil:Lalouche:2016} software corpus,
assembled from~26 popular \Java open source software artifacts collected
from~\emph{GitHub's Trending
  repositories}~\urlref{https://github.com/trending?l=java＆since=monthly} and
the \emph{GitHub \Java Corpus}%
\urlref{http://groups.inf.ed.ac.uk/cup/javaGithub/}
list due to Charles and Allamanis~\cite{Charles:Allamanis:2013}

The ‟Size Before” column of \Cref{table:corpus} provides the main size
statistics of the constituting artifacts.

\begin{table}[H]
  \caption{Aggregating statistics of compression power of
    BZip2 and size of software artifacts
  corpus before and after compression}
  \label{table:corpus}
  \par\vspace{10pt plus 6pt minus 4pt}
  \centering
  \begin{adjustbox}{max width=\columnwidth}
    \scriptsize
    \begin{tabular}{l*3r}
      \toprule
      & \multicolumn{2}{c}{\textit{Size (bytes)}}⏎
      \cmidrule(r){2-3}
      & \textit{Before}
      & \textit{After}
      & \multicolumn1c{\textit{Power}}⏎
      \midrule % VIM: +,/bottom/-!column -t|sed 's/^/    /'
    \sffamily  Ave\@.  ＆  12,370,223  &  1,272,331  &  9.25   ⏎
    \sffamily  Min\@.  ＆  1,019,125   &  129,114    &  7.22   ⏎
    \sffamily  Max\@.  ＆  46,965,422  &  5,607,599  &  12.96  ⏎
    \sffamily  Med\@.  ＆  5,128,541   &  607,535    &  8.91   ⏎
    \sffamily  Range   &   45,946,297  &  5,478,485  &  5.74   ⏎
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}

Notably, sizes span more than one order of magnitude, from one megabyte of
source to circa~47, the median being around five megabytes, the average around
twelve. The GL corpus was used before for the study of software metrics.
See~\cite{Gil:Lalouche:2016}~\cite{Cite:Gal:SecondPaper}\matteo for
reproducibility details and for a description of the artifacts' selection
process. Sufficient to us here is that the selection was independent of the
current research and that the artifacts are of considerable size, wide use,
evolution history and, development effort.

\subsection{Burrows-Wheeler Compression}
Compression power is used in this work-in-progress report as a quick estimate
of software's naturalness~\cite{Hindle:Bar:Su:Gabel:Devanbu:2012}. The
measures are similar but not identical: Compression algorithms search
(and exploit) the input for repetitive patterns of unbounded length, while
naturalness limits the search to~$n$-grams for some small integer~$n$. Quality
in compression algorithms is measured by compression \emph{power}, defined as
the ratio of uncompressed size to the compressed size. Compression power should
be correlated with \emph{naturalness}, but it distinct from it.

We found that with respect to \Java software the compression power of Bzip2 is
greater than that of GZip. This greater power is probably because Gzip make a
greedy search for repetitions starting at the input's prefix, while BZip2
starts the search in a broader context, as implied by the Burrows-Wheeler
algorithm~\cite{Burrows:Wheeler:94}.

Columns~2 and~3 of \cref{table:corpus} provide statistics of compressed size and
compression power with BZip2\urlref{http://bzip.org/} compression. As in the
study of naturalness of software, we witness the fact that projects can be very
different. There is two fold range variation of their associated compression
power, as there is in their naturalness.

\subsection{The Jack Preprocessor}

To better focus on ‟pure” code complexity, our study further employs
\emph{Jack}, a special purpose preprocessor. Jack replaces each keyword and
operator with a single one byte token. The rationale is that keywords such as
\kk{class} are read and produced by programmers as an individual cohesive
token, rather than a sequence of letters that form an identifier, or sequence
of digits that make a numeric literal.

Jack also eliminates from the \Java code all of the following five: white space
characters, \kk{package} declaration, \kk{import} directives, comments of all
sorts, and, the body of all string literals. The assumption here is that each
of these is not a product of pure coding effort: White space characters are
generated and frequently change by automatic code formatting. Also, \kk{import}
directives are generated and optimized automatically, and are subject to
underlying project guideline. Comments also follow their own style, which is
very different from that of the code, \emph{and}, string literals, at least
those with significant content, are ought to be managed by configuration and
internationalization processes rather than coding per-se.

Jack currently does not remove unit-tests, but is planned to do so in the
future.

\Cref{table:original}
our dataset. After removing the unessential code and obtaining boilerplate code
we computed compression power. We also applied the Burrows-Wheeler algorithm to
the outcome of the Jack algorithm. As we can see if we compare the results
reported in \cref{table:original} and \cref{table:original} the compression
power is higher if the BW algorithm is applied to the boilerplate code.

\begin{table}
  \matteo†{copy and adapt from first caption}
  \caption{The results of the compression using the Jack algorithm alone and
  both Jack and Burrows-Wheeler algorithms combined}
  \label{table:original}
  \par\vspace{10pt plus 6pt minus 4pt}
  \centering
  \scalebox{0.75}{%
    \begin{tabular}{l*5r}
      \toprule
      & \multicolumn{2}{c}{\textit{Jack}}
      & \multicolumn{2}{c}{\textit{Jack + BZip2}}
      & \textit{Combined-}⏎
      \cmidrule(r){2-3} \cmidrule(r){4-5}
                      & \textit{Size (bytes)} 
                      & \textit{Ratio} 
                      & \textit{Size (bytes)} 
                      & \textit{Ratio} & \textit{-Power} ⏎
    \midrule % VIM: +,/bottom/-!column -t|sed 's/^/    /'
    \sffamily  Ave.   &  5,078,034   &  2.78   &  729,934    &  6.60  &  18.37⏎
    \sffamily  Min.   &  456,050     &  1.73   &  77,706     &  4.89  &  10.18⏎
    \sffamily  Max.   &  19,697,634  &  12.85  &  3,414,889  &  8.68  &  83.65⏎
    \sffamily  Med.   &  1,822,510   &  2.34   &  343,815    &  6.50  &  15.95⏎
    \sffamily  Range  &  19,241,584  &  11.12  &  3,337,183  &  3.79  &  73.47⏎
      \bottomrule
    \end{tabular}}
\end{table}

\subsection{Pure Spartanization}

We than applied a set of spartanizations (partial spartanization) to all the
software systems. Afterwards we applied both the Jack and the BW algorithm.
The results are reported in \cref{table:partial} where it is also
reported the improvement in the compression due to the spartanization. It is
worth to note that we have an improvement in any statistics, in other words the
spartanization affected positively the compression, increasing the compression
power. This is counter intuitive, being a small file most difficult to
compress than a larger one (REFORMULATE)

\begin{table}
  \matteo†{copy and adapt from first caption}
  \caption{The results after performing a partial spartanization of the code,
    compared with the compression ratios obtained with the Jack and
  Burrows-Wheeler algorithm}
  \label{table:partial}
  \par\vspace{10pt plus 6pt minus 4pt}
  \centering
  \begin{tabular}{l*5r}
    \toprule
    & \multicolumn{2}{c}{\textit{Size (bytes)}} 
    & \textit{Ratio} 
    &\multicolumn{2}{c}{\textit{Improvement}}⏎
    \cmidrule(r){2-3} \cmidrule(r){5-6}
                    & \textit{Jack} 
                    & \textit{Jack + BZip2} 
                    & & \textit{with Spartan.} & \textit{Total}⏎
    \midrule % VIM: +,/bottom/-!column -t|sed 's/^/    /'
    \sffamily  Ave\@.  &  5,050,647   &  719,488    &  6.66  &  0.06   &  1.01  ⏎
    \sffamily  Min\@.  &  439,825     &  75,391     &  4.90  &  -0.37  &  0.21  ⏎
    \sffamily  Max\@.  &  19,173,224  &  3,275,918  &  8.79  &  0.49   &  1.07  ⏎
    \sffamily  Med\@.  &  1,959,917   &  341,851    &  6.79  &  0.08   &  1.04  ⏎
    \sffamily  Range   &  18,733,399  &  3,200,527  &  3.89  &  0.86   &  0.87  ⏎
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Renaming Spartanization}
\matteo†{copy and adapt from first caption}
\begin{table}
  \caption{The results after performing a total spartanization of the code,
    compared with the compression ratios obtained with the Jack and
  Burrows-Wheeler algorithm.}
  \label{table:total}
  \par\vspace{10pt plus 6pt minus 4pt}
  \centering
  \scalebox{0.70}{%
    \begin{tabular}{l*5r}
      \toprule
      & \multicolumn{2}{c}{\textit{Size (bytes)}} 
      & \textit{Ratio} 
      &\multicolumn{2}{c}{\textit{Improvement over}}⏎
      \cmidrule(r){2-3} \cmidrule(r){5-6}
                      & \textit{Jack} 
                      & \textit{Jack + BZip2} 
                      &      & \textit{Overall} 
                      & \textit{Partial Spartan.}⏎
      \midrule % VIM: +,/bottom/-!column -t|sed 's/^/    /'
    \sffamily  Ave.   &  4,947,319   &  713,857    &  6.56  &  -0.03  &  -0.09⏎
    \sffamily  Min.   &  430,375     &  75,160     &  4.89  &  -0.40  &  -0.28⏎
    \sffamily  Max.   &  18,890,899  &  3,244,851  &  8.72  &  0.41   &  -0.01⏎
    \sffamily  Med.   &  1,923,056   &  339,930    &  6.70  &  0.00   &  -0.09⏎
    \sffamily  Range  &  18,460,524  &  3,169,691  &  3.83  &  0.81   &  0.27⏎
      \bottomrule
    \end{tabular}}
\end{table}

Preliminary results supporting the
‟Hypothesis that can never be proved”
Because this is not what you like.
