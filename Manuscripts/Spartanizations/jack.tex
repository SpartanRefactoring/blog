% Then apply Burroughs-Wheeler, explain why better than zip.
% For entire project.

The experiment is based on two compression algorithms.
The first compression algorithm is called Jack, and it rips off from the file all the code note related to the programming logic, leaving
only the code intentionally written by the developers, namely the so called boilerplate code.

Specifically, in order to do that, Jack automatically removes:

\begin{itemize}
  \item imports instructions
  \item package
  \item comments
  \item strings perhaps use
  \item keywords are replaced by a single symbol rep
        but not identifiers. Take note that it is common wisdom that the bulk of code is identifiers.
  \item same for multi-character operators such as \cc{->}, \cc{[]}, and \cc{<<<<=}
\end{itemize}

The second compression algorithm was invented by Burroughs and Wheeler
\cite{Burrows:Wheeler:94}. In its first stage, this algorithm performs a
permutation of the characters based on their frequency, increasing the
compression ratio.

The data corpus used in the experiments is the assembles~26 popular \Java open
source software artifacts found on~\emph{GitHub's Trending
repositories}~\urlref{https://github.com/trending?l=java＆since=monthly} or in
the \emph{GitHub \Java Corpus} list due to Charles and
Allamanis~\cite{Charles:Allamanis:2013}\urlref{http://groups.inf.ed.ac.uk/cup/javaGithub/}.
The ‟Size Before” column \Cref{table:corpus} provides main size statistics of
the constituting artifacts.

\begin{table}
  \label{table:corpus}
  \caption{The results of the compression using the Burroughs-Wheeler algorithm}
  \centering
  \scalebox{0.8}{
    \begin{tabular}{l*3r}
      \toprule
      & \multicolumn{2}{c}{\textit{Size (bytes)}} & \multicolumn1c{\textit{Compression-}}⏎
      \cmidrule(r){2-3}
                       & \textit{Before} & \textit{After} & \multicolumn1c{\textit{-factor}}⏎
      \midrule
      \itshape Average & 12,370,223 & 1,272,331 & 9.25⏎
      \itshape Minimum & 1,019,125 & 129,114 & 7.22⏎
      \itshape Maximum & 46,965,422 & 5,607,599 & 12.96⏎
      \itshape Range & 45,946,297 & 5,478,485 & 5.74⏎
      \itshape Median & 5,128,541 & 607,535 & 8.91⏎
      \bottomrule
    \end{tabular}}
\end{table}

We see that sizes span more than one order of magnitude, from one megabyte of
source to circa~47, the median being around 5MB. The corpus was used before for
the study of software metrics. See~\cite{Gil:Lalouche:2016}~\cite{Matteo:Cite:Gal:SecondPaper} for
reproducibility details and for a description of the artifacts' selection
process. Sufficient to us here is that the selection was independent of the
current research and that the artifacts are of considerable size, wide
use, and, development effort.
