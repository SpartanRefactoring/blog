We used the ``Gil-Lalouche''~\cite{Gil:Lalouche:2016} software corpus,
assembled from~26 popular \Java open source software artifacts collected
from~\emph{GitHub's Trending
repositories}~\urlref{https://github.com/trending?l=java＆since=monthly} and 
the \emph{GitHub \Java Corpus} list due to Charles and
Allamanis~\cite{Charles:Allamanis:2013}\urlref{http://groups.inf.ed.ac.uk/cup/javaGithub/}.

The ‟Size Before” column of \Cref{table:corpus} provides the main size
statistics of the constituting artifacts.

\begin{table}
  \label{table:corpus}
  \caption{Aggregating statistics of compression power of BZip2 and size of software artifacts 
  corpus before and after compression}
    \mbox{}\newline
    \mbox{}\newline
  \centering
  \begin{adjustbox}{max width=\columnwidth}
    \scriptsize
    \begin{tabular}{l*3r}
      \toprule
      & \multicolumn{2}{c}{\textit{Size (bytes)}}  ⏎
      \cmidrule(r){2-3}
                       & \textit{Before} & \textit{After} & \multicolumn1c{\textit{Power}}⏎
      \midrule
      \sffamily Ave.\ & 12,370,223 & 1,272,331 & 9.25⏎
      \sffamily Min.\ & 1,019,125 & 129,114 & 7.22⏎
      \sffamily Max.\ & 46,965,422 & 5,607,599 & 12.96⏎
      \sffamily Med.\ & 5,128,541 & 607,535 & 8.91⏎
      \sffamily Range & 45,946,297 & 5,478,485 & 5.74⏎
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}

Notably, sizes span more than one order of magnitude, from one megabyte of
source to circa~47, the median being around five megabytes, the average around
twelve.  The GL corpus was used before for the study of software metrics.
See~\cite{Gil:Lalouche:2016}~\cite{Cite:Gal:SecondPaper}\matteo for
reproducibility details and for a description of the artifacts' selection
process. Sufficient to us here is that the selection was independent of the
current research and that the artifacts are of considerable size, wide use,
evolution history and, development effort.

Compression power is used in this work-in-progress report as a quick estimate
of software's naturalness~\cite{Hindle:Bar:Su:Gabel:Devanbu:2012}.  The
measures are similar but not identical: Compression algorithms search
(and exploit) the input for repetitive patterns of unbounded length, while
naturalness limits the search to~$n$-grams for some small integer $n$.  Quality
in compression algorithms is measured by compression \emph{power}, defined as
the ratio of uncompressed size to the compressed size. Compression power should
be correlated with \emph{naturalness}, but it distinct from it.

We found that Bzip2 is a better compressor than GZip. This is probably because
Gzip make a greedy search for repetitions starting at the input's prefix, while
BZip2 starts the search in a broader context, as implied by the
Burrows-Wheeler algorithm~\cite{Burrows:Wheeler:94}. 

Columns~2 and~3 of \cref{table:corpus} provide statistics of size and
compression power after BZip2\urlref{http://bzip.org/} compression. 



The experiment is based on two compression algorithms.
The first compression algorithm is called Jack, and it rips off from the file all the code note related to the programming logic, leaving
only the code intentionally written by the developers, namely the so called boilerplate code.

Specifically, in order to do that, Jack automatically removes:

\begin{itemize}
  \item imports instructions
  \item package
  \item comments
  \item strings perhaps use
  \item keywords are replaced by a single symbol rep
        but not identifiers. Take note that it is common wisdom that the bulk of code is identifiers.
  \item same for multi-character operators such as \cc{->}, \cc{[]}, and \cc{<<<<=}
\end{itemize}

The second compression algorithm was invented by Burrows and Wheeler
\cite{Burrows:Wheeler:94}. In its first stage, this algorithm performs a
permutation of the characters based on their frequency, increasing the
compression power.


